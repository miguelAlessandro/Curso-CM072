{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobreajuste \n",
    "\n",
    "* ¿Qué es sobreajustado?\n",
    "     - Construir un modelo que coincida con los datos de entrenamiento \"muy de cerca\",que los memorize.\n",
    "\n",
    "* ¿Cómo ocurre el sobreajuste?\n",
    "    - Evaluaando un modelo probándolo con los mismos datos que se usaron para entrenarlo.\n",
    "    - Creando un modelo que es \"demasiado complejo\"\n",
    "\n",
    "* ¿Cuál es el impacto del sobreajuste?\n",
    "    - El modelo funcionará bien en los datos de entrenamiento, pero no se generalizará a datos no conocidos.\n",
    "    - El modelo tendrá un sesgo bajo, pero una gran varianza.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobreajuste en  modelos lineales\n",
    "\n",
    "* ¿Cuáles son las características generales de los modelos lineales?\n",
    "\n",
    "   * Baja complejidad de modelo\n",
    "   * Alto sesgo, baja varianza\n",
    "   * No tiende al sobreajuste\n",
    "\n",
    "Sin embargo, el sobreajuste con modelos lineales ocurre si permite que tengan alta varianza. Aquí hay algunas causas comunes:\n",
    "\n",
    "* Características irrelevantes\n",
    "\n",
    "Los modelos lineales pueden sobreajustarse si incluye \"características irrelevantes\", es decir, características que no están relacionadas con la respuesta. ¿Por qué? \n",
    "\n",
    "Porque el modelo aprenderá un coeficiente para cada característica  independientemente de si esa característica tiene la señal o el ruido.\n",
    "\n",
    "Esto es especialmente un problema cuando `p` (número de características) está cerca de `n` (número de observaciones), porque ese modelo naturalmente tendrá alta varianza.\n",
    "\n",
    "* Características correlacionadas\n",
    "\n",
    "Los modelos lineales pueden sobreajustarse si las características incluidas están altamente correlacionadas entre sí. ¿Por qué?\n",
    "\n",
    "De la documentación de scikit-learn:\n",
    "\n",
    "    \"... las estimaciones de coeficientes para mínimos cuadrados  dependen de la independencia de los términos del modelo. Cuando los términos están correlacionados y las columnas de la matriz  X tienen una dependencia lineal aproximada, la matriz se vuelve casi singular y como resultado, la estimación de mínimos cuadrados se vuelve altamente sensible a errores aleatorios en la respuesta observada, produciendo una gran varianza \".\n",
    "\n",
    "* Coeficientes grandes\n",
    "\n",
    "Los modelos lineales pueden sobreajustarse si los coeficientes (después de la estandarización de características) son demasiado grandes. ¿Por qué?\n",
    "\n",
    "Debido a que cuanto mayor es el valor absoluto del coeficiente, más poder tiene para cambiar la respuesta predicha, lo que resulta en una mayor varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización de modelos lineales\n",
    "\n",
    "* La regularización es un método para \"restringir\" o \"regularizar\" el tamaño de los coeficientes, \"encogiéndolos\" hacia cero.\n",
    "* Reduce la varianza del modelo y, por lo tanto, minimiza el sobreajuste.\n",
    "* Si el modelo es demasiado complejo, tiende a reducir la varianza más de lo que aumenta el sesgo, lo que da como resultado un modelo que es más probable que generalice nuevos datos.\n",
    "\n",
    "Para un modelo de regresión lineal normal, estimamos los coeficientes utilizando el criterio de mínimos cuadrados, que minimiza la suma de cuadrados residuales (RSS). \n",
    "\n",
    "Para un modelo de regresión lineal regularizado, minimizamos la suma de RSS y un \"término de penalización\" que penaliza el tamaño del coeficiente.\n",
    "\n",
    "La regresión de Ridge (o \"regularización  L2\") minimiza:\n",
    "\n",
    "$$RSS +\\alpha\\sum_{j =1}^{p}w_j^2$$\n",
    "\n",
    "La regresión Lasso (o \"regularización  L1\") minimiza:\n",
    "\n",
    "$$RSS +\\alpha\\sum_{j =1}^{p}\\vert w_j\\vert$$\n",
    "\n",
    "\n",
    "* $p$ es el número de características.\n",
    "* $w_j$ es el coeficiente del modelo.\n",
    "* $\\alpha$ es el parámetro de ajuste.\n",
    "    - Un pequeño $\\alpha $ no impone ninguna penalización en el tamaño del coeficiente, y es equivalente a un modelo de regresión lineal normal.\n",
    "    - Aumentar el $\\alpha $ penaliza los coeficientes y por lo tanto, los reduce.\n",
    "    \n",
    "    \n",
    "Un alfa más grande da como resultado una mayor regularización:\n",
    "\n",
    "   - La regresión Lasso reduce los coeficientes hasta cero, eliminándolos del modelo.\n",
    "   - La regresión Ridge reduce los coeficientes hacia cero, pero raramente llegan a cero.\n",
    "     \n",
    "     \n",
    "### Como utilizar la regularización\n",
    "\n",
    "* ¿Deberían estandarizarse las características?\n",
    "\n",
    "    - Sí, porque de lo contrario, las características se penalizarían simplemente por su escala.\n",
    "    - Además, la estandarización evita penalizar el intercepto, lo que no tendría sentido intuitivo.\n",
    "\n",
    "* ¿Cómo se debe elegir entre regresión Lasso y regresión Ridge?\n",
    "\n",
    "     - Se prefiere la regresión Lasso si creemos que muchas características son irrelevantes o si preferimos un modelo disperso.\n",
    "     - Si el rendimiento del modelo es tu principal preocupación, lo mejor es probar ambos.\n",
    "     - La regresión de ElasticNet es una combinación de la regresión Lasso y regresión Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de la regularización\n",
    "\n",
    "A continuación se muestra una visualización de lo que sucede cuando se aplica la regularización. La idea general es que se está restringiendo los valores permitidos de los coeficientes a cierta \"región\". Dentro de esa región, se requiere encontrar los coeficientes que resultan en el mejor modelo.\n",
    "\n",
    "![](K1.png)\n",
    "\n",
    "En este diagrama:\n",
    "\n",
    "* Estamos ajustando un modelo de regresión lineal con dos características, $x_1$ y $x_2$.\n",
    "* $\\hat\\beta $ representa el conjunto de dos coeficientes, $\\beta_1$ y $\\beta_2 $, que minimizan el RSS para el modelo no regularizado.\n",
    "* La regularización restringe las posiciones permitidas de $\\hat\\beta $ a la región de restricción verde:\n",
    "    * Para Lasso, esta región es un diamante porque restringe el valor absoluto de los coeficientes.\n",
    "    * Para cresta, esta región es un círculo porque restringe el cuadrado de los coeficientes.\n",
    "   \n",
    "* El tamaño de la región verde viene determinado por $\\alpha $, con un menor $\\alpha $ que da como resultado una región más grande:\n",
    "    * Cuando $\\alpha $ es cero, la región verde es infinitamente grande y, por lo tanto, los tamaños de los coeficientes no están limitados.\n",
    "    * Cuando $\\alpha $ aumenta, la región  verde se hace más y más pequeña.\n",
    "\n",
    "En este caso, $\\hat\\beta $ no está dentro de la región verde de restricción. Por lo tanto, debemos mover $\\hat\\beta $ hasta que interseque la región verde, mientras aumentamos el RSS lo menos posible.\n",
    "\n",
    "Del libro de Gareth James, Daniela Witten, Trevor Hastie y Robert Tibshirani: [An Introduction to\n",
    "Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf):\n",
    "\n",
    "Las elipses que se centran en $\\hat\\beta$ representan regiones de RSS constante. En otras palabras, todos los puntos en una elipse dada comparten un valor común del RSS. A medida que las elipsis se expanden lejos de las estimaciones del coeficiente de mínimos cuadrados, el RSS aumenta. Las ecuaciones (6.8) y (6.9) indican que las estimaciones del coeficiente de regresión de Lasso y Ridge están dadas por el primer punto en el que una elipse entra en contacto con la región de restricción.\n",
    "\n",
    "Como la regresión Ridge tiene una restricción circular sin puntos agudos, esta intersección generalmente no ocurrirá en un eje, por lo que las estimaciones del coeficiente de regresión Ridge serán exclusivamente distintas de cero. Sin embargo, la restricción Lasso tiene esquinas en cada uno de los ejes, por lo que la elipse a menudo intersectará la región de restricción en un eje. \n",
    "    \n",
    "Cuando esto ocurre, uno de los coeficientes será igual a cero. En dimensiones más altas, muchas de las estimaciones de coeficientes pueden ser iguales a cero simultáneamente. En la anterior la intersección se produce en $\\beta_1 = 0 $, por lo que el modelo resultante solo incluirá  $\\beta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizacion con scikit-learn\n",
    "\n",
    "\n",
    "* Ejemplo: Predecir la tasa de delitos violentos para una comunidad dada la información socioeconómica y de aplicación de la ley, utilizando los datos: [communities.data](http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lakewoodcity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tukwilacity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aberdeentown</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>5.0</td>\n",
       "      <td>81440.0</td>\n",
       "      <td>Willingborotownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>95.0</td>\n",
       "      <td>6096.0</td>\n",
       "      <td>Bethlehemtownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1        2                    3    4     5     6     7     8     9    \\\n",
       "0    8   NaN      NaN         Lakewoodcity    1  0.19  0.33  0.02  0.90  0.12   \n",
       "1   53   NaN      NaN          Tukwilacity    1  0.00  0.16  0.12  0.74  0.45   \n",
       "2   24   NaN      NaN         Aberdeentown    1  0.00  0.42  0.49  0.56  0.17   \n",
       "3   34   5.0  81440.0  Willingborotownship    1  0.04  0.77  1.00  0.08  0.12   \n",
       "4   42  95.0   6096.0    Bethlehemtownship    1  0.01  0.55  0.02  0.95  0.09   \n",
       "\n",
       "   ...    118   119   120   121   122  123  124   125   126   127  \n",
       "0  ...   0.12  0.26  0.20  0.06  0.04  0.9  0.5  0.32  0.14  0.20  \n",
       "1  ...   0.02  0.12  0.45   NaN   NaN  NaN  NaN  0.00   NaN  0.67  \n",
       "2  ...   0.01  0.21  0.02   NaN   NaN  NaN  NaN  0.00   NaN  0.43  \n",
       "3  ...   0.02  0.39  0.28   NaN   NaN  NaN  NaN  0.00   NaN  0.12  \n",
       "4  ...   0.04  0.09  0.02   NaN   NaN  NaN  NaN  0.00   NaN  0.03  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos los datos\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "datos_crimen = pd.read_csv('communities.data', header=None, na_values=['?'])\n",
    "datos_crimen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1994.000000\n",
       "mean        0.237979\n",
       "std         0.232985\n",
       "min         0.000000\n",
       "25%         0.070000\n",
       "50%         0.150000\n",
       "75%         0.330000\n",
       "max         1.000000\n",
       "Name: 127, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examinamos la variable respuesta\n",
    "datos_crimen[127].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removemos la variables categoricas\n",
    "datos_crimen.drop([0, 1, 2, 3, 4], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removemos las filas con valores perdidos\n",
    "datos_crimen.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 123)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forma  de los datos\n",
    "datos_crimen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos X e y\n",
    "X = datos_crimen.drop(127, axis=1)\n",
    "y = datos_crimen[127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba= train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construimos el modelo de regresión lineal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_entrenamiento, y_entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.66188167e+00  6.98124465e-01 -2.61955467e-01 -2.85270027e-01\n",
      " -1.64740837e-01  2.46972333e-01 -1.09290051e+00 -5.96857796e-01\n",
      "  1.11200239e+00 -7.21968931e-01  4.27346598e+00 -2.28040268e-01\n",
      "  8.04875769e-01 -2.57934732e-01 -2.63458023e-01 -1.04616958e+00\n",
      "  6.07784197e-01  7.73552561e-01  5.96468029e-02  6.90215922e-01\n",
      "  2.16759430e-02 -4.87802949e-01 -5.18858404e-01  1.39478815e-01\n",
      " -1.24417942e-01  3.15003821e-01 -1.52633736e-01 -9.65003927e-01\n",
      "  1.17142163e+00 -3.08546690e-02 -9.29085548e-01  1.24654586e-01\n",
      "  1.98104506e-01  7.30804821e-01 -1.77337294e-01  8.32927588e-02\n",
      "  3.46045601e-01  5.01837338e-01  1.57062958e+00 -4.13478807e-01\n",
      "  1.39350802e+00 -3.49428114e+00  7.09577818e-01 -8.32141352e-01\n",
      " -1.39984927e+00  1.02482840e+00  2.13855006e-01 -6.18937325e-01\n",
      "  5.28954490e-01  7.98294890e-02  5.93688560e-02 -1.68582667e-01\n",
      "  7.31264051e-01 -1.39635208e+00  2.38507704e-01  5.50621439e-01\n",
      " -5.61447867e-01  6.18989764e-01  2.55517024e+00 -3.71769599e+00\n",
      "  7.09191935e-01  3.82041439e-01  8.23752836e-01 -1.67703547e+00\n",
      " -1.73150450e+00  9.90120171e-01 -5.72745697e-01 -1.45877295e+00\n",
      "  8.68032144e-01  5.15959984e-01  3.14453207e-02  2.01869791e-01\n",
      "  9.65291940e-02  2.13034099e+00 -6.95374423e-02  4.62477023e-02\n",
      " -1.10565955e-02 -1.34313780e-02 -1.04515494e-01 -8.76985171e-01\n",
      "  4.26781907e-01 -1.85405795e-01 -8.16215517e-01 -2.86596076e-01\n",
      " -1.56110708e-01  1.76468580e+00 -5.70163730e-01 -7.54066704e-02\n",
      " -1.74212697e-01 -8.89747220e-02  2.26336403e-01  1.38030073e+00\n",
      " -3.37304744e-01 -2.57856611e-02  8.91299188e-02  3.49876793e-01\n",
      " -1.22428557e+00 -3.67941205e+01 -6.95699750e-01  2.95269279e-01\n",
      " -1.48590316e-03  2.34206416e-01 -7.09533984e-03  3.67152957e+01\n",
      " -8.90665109e-02  3.79550678e-02  3.19375782e-01  4.60708905e-01\n",
      "  1.41090069e-01 -6.67017320e-01 -2.59035245e-01 -4.60600755e-04\n",
      " -1.51868232e-02  7.54768410e-02 -2.36105498e-03 -1.50328233e-01\n",
      "  1.85575558e-01  6.31979224e-01 -1.50253625e-01  1.87638817e-02\n",
      " -3.38095851e-02 -4.46104032e-01]\n"
     ]
    }
   ],
   "source": [
    "# Examinamos los coeficientes\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos predicciones\n",
    "y_pred = linreg.predict(X_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2338136764948703\n"
     ]
    }
   ],
   "source": [
    "# Calculamos RMSE\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "print (np.sqrt(metrics.mean_squared_error(y_prueba, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Ridge\n",
    "\n",
    "* Documentación de Ridge:[sklearn.linear_model.Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html).\n",
    "* Parámetros\n",
    "    * alpha: debe ser positivo, creciente para obtener más regularización.\n",
    "    * normalize: escala las características (sin usar StandardScaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2338136764947959\n"
     ]
    }
   ],
   "source": [
    "# alpha=0 es equivalente a una regresion lineal\n",
    "from sklearn.linear_model import Ridge\n",
    "ridgereg = Ridge(alpha=0, normalize=True)\n",
    "ridgereg.fit(X_entrenamiento, y_entrenamiento)\n",
    "y_pred = ridgereg.predict(X_prueba)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_prueba, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16427906804924075\n"
     ]
    }
   ],
   "source": [
    "# Probamos alpha=0.1\n",
    "ridgereg = Ridge(alpha=0.1, normalize=True)\n",
    "ridgereg.fit(X_entrenamiento, y_entrenamiento)\n",
    "y_pred = ridgereg.predict(X_prueba)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_prueba, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.00298418e-03  3.51647445e-02  6.03535935e-02 -7.68532502e-02\n",
      " -1.76099849e-02  4.53791433e-02  8.81586468e-03 -2.88885814e-02\n",
      " -1.92143587e-02  3.36122201e-02  5.71590736e-04 -4.85438136e-02\n",
      "  5.55725157e-02 -1.15934270e-01 -1.11880845e-01 -3.32742094e-01\n",
      " -1.12302031e-02  9.63833243e-02 -8.92057732e-02  8.42691702e-02\n",
      " -1.67246717e-02  7.42520308e-03 -1.21294025e-01 -6.70155789e-02\n",
      " -1.74250249e-03  1.69446833e-01  3.18217654e-02 -1.00209834e-01\n",
      "  3.97535644e-02 -1.19173054e-01 -1.04445267e-01 -5.14946676e-03\n",
      "  1.10071013e-01 -3.22958955e-02 -1.40601627e-01  7.72658029e-02\n",
      "  9.07962536e-02 -3.78878862e-03  4.61941793e-02  6.30299731e-02\n",
      " -3.09236932e-02  1.02883578e-02  9.70425568e-02 -1.28936944e-01\n",
      " -1.38268907e-01 -6.37169778e-02 -8.80160419e-02 -4.01991014e-02\n",
      "  8.11064596e-02 -6.30663975e-02  1.29756859e-01 -6.25210624e-02\n",
      "  1.60531213e-02 -1.39061824e-01  6.39822353e-02  4.87118744e-02\n",
      " -7.68217532e-03 -1.53523412e-03  1.73028280e-02 -1.37258659e-03\n",
      " -1.97381922e-02  4.47492477e-02  3.53941624e-03 -1.64126843e-02\n",
      " -1.62363185e-02  7.10860268e-02 -1.34543849e-01  3.03401863e-02\n",
      "  2.87012058e-02  2.62507811e-01  3.87946361e-02  4.16976393e-02\n",
      "  2.45959130e-02  4.02803695e-02 -1.15568319e-02  1.82352709e-02\n",
      " -1.11769965e-04  1.17220288e-02 -3.27960499e-02 -2.06336390e-02\n",
      " -2.01424775e-02 -1.55746075e-02 -1.50471159e-01  5.00237268e-02\n",
      "  1.67270388e-02  1.27989507e-01 -7.55437715e-02 -7.22756020e-02\n",
      " -8.80283128e-02  6.42301728e-02  1.39781081e-01  4.71861289e-02\n",
      " -6.42667056e-02  3.16227166e-02 -1.36066226e-02  5.16507328e-02\n",
      " -4.60206271e-02  6.55072592e-04  3.51488294e-02 -1.68717518e-02\n",
      " -7.00033520e-03  4.99335627e-02  8.40464679e-02  3.87553978e-03\n",
      " -1.23632746e-01 -2.24505480e-02 -2.47960018e-03  4.13468551e-02\n",
      "  8.26295505e-02 -4.84167513e-02  8.21329530e-03  1.57843967e-02\n",
      " -1.94698620e-02  4.09120489e-02 -4.42911592e-02 -5.64373896e-02\n",
      "  1.17841094e-01  7.34994342e-02 -2.78153968e-02  3.74136314e-02\n",
      " -7.67878399e-02 -4.65440973e-02]\n"
     ]
    }
   ],
   "source": [
    "# Examinamos los coeficientes\n",
    "print(ridgereg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RidgeCV\n",
    "\n",
    "Regresión Ridge con validación cruzada incorporada del parámetro `alpha`.\n",
    "  \n",
    "  * Documentación de RidgeCV: [sklearn.linear_model.RidgeCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html).\n",
    "  * alphas: matriz de valores `alpha` (elegidos automáticamente) para probar.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rango_alpha = 10.**np.arange(-2, 3)\n",
    "rango_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seleccionamos el mejor alpha con  RidgeCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "ridgeregcv = RidgeCV(alphas=rango_alpha, normalize=True, scoring='neg_mean_squared_error')\n",
    "ridgeregcv.fit(X_entrenamiento, y_entrenamiento)\n",
    "ridgeregcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16312978234269246\n"
     ]
    }
   ],
   "source": [
    "# El método de predict utiliza el mejor valor alfa\n",
    "y_pred = ridgeregcv.predict(X_prueba)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_prueba, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Lasso\n",
    "\n",
    "* Documentación de Lasso:[sklearn.linear_model.Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html).\n",
    "* Parámetros\n",
    "    * alpha: debe ser positivo, creciente para obtener más regularización.\n",
    "    * normalize: escala las características (sin usar StandardScaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.00891952 -0.27423369  0.          0.\n",
      "  0.         -0.         -0.          0.          0.          0.\n",
      " -0.         -0.         -0.         -0.19414627  0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.          0.          0.          0.          0.04335664 -0.\n",
      "  0.         -0.          0.03491474 -0.         -0.06685424  0.\n",
      "  0.         -0.          0.10575313  0.          0.          0.00890807\n",
      "  0.         -0.1378172  -0.30954312 -0.         -0.         -0.\n",
      " -0.          0.          0.          0.          0.         -0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.          0.          0.          0.         -0.          0.\n",
      " -0.         -0.          0.          0.05257892 -0.          0.\n",
      " -0.         -0.          0.          0.          0.          0.\n",
      "  0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.          0.         -0.         -0.          0.\n",
      "  0.13861081  0.         -0.         -0.          0.          0.\n",
      "  0.          0.         -0.          0.          0.          0.\n",
      "  0.03347908  0.         -0.01130055 -0.          0.          0.\n",
      "  0.00044205  0.          0.          0.         -0.          0.\n",
      " -0.         -0.          0.04153636  0.         -0.          0.00719672\n",
      " -0.000666    0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Probamos alpha=0.001 y examinamos los coeficientes\n",
    "from sklearn.linear_model import Lasso\n",
    "lassoreg = Lasso(alpha=0.001, normalize=True)\n",
    "lassoreg.fit(X_entrenamiento, y_entrenamiento)\n",
    "print(lassoreg.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.00891952 -0.27423369  0.          0.\n",
      "  0.         -0.         -0.          0.          0.          0.\n",
      " -0.         -0.         -0.         -0.19414627  0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.          0.          0.          0.          0.04335664 -0.\n",
      "  0.         -0.          0.03491474 -0.         -0.06685424  0.\n",
      "  0.         -0.          0.10575313  0.          0.          0.00890807\n",
      "  0.         -0.1378172  -0.30954312 -0.         -0.         -0.\n",
      " -0.          0.          0.          0.          0.         -0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.          0.          0.          0.         -0.          0.\n",
      " -0.         -0.          0.          0.05257892 -0.          0.\n",
      " -0.         -0.          0.          0.          0.          0.\n",
      "  0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.          0.         -0.         -0.          0.\n",
      "  0.13861081  0.         -0.         -0.          0.          0.\n",
      "  0.          0.         -0.          0.          0.          0.\n",
      "  0.03347908  0.         -0.01130055 -0.          0.          0.\n",
      "  0.00044205  0.          0.          0.         -0.          0.\n",
      " -0.         -0.          0.04153636  0.         -0.          0.00719672\n",
      " -0.000666    0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Probamos alpha=0.01 y examinamos los coeficientes\n",
    "from sklearn.linear_model import Lasso\n",
    "lassoreg = Lasso(alpha=0.001, normalize=True)\n",
    "lassoreg.fit(X_entrenamiento, y_entrenamiento)\n",
    "print(lassoreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16003902404387876\n"
     ]
    }
   ],
   "source": [
    "# calculamos RMSE (para alpha=0.01)\n",
    "y_pred = lassoreg.predict(X_prueba)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_prueba, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoCV\n",
    "\n",
    "Regresión Lasso con validación cruzada incorporada del parámetro `alpha`.\n",
    "\n",
    "   * Documentación de LassoCV: [sklearn.linear_model.LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html).\n",
    "   * n_alphas: número de valores `alpha` (elegidos automáticamente) para probar.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015161594598125873"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seleccionamos el mejor alpha con LassoCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "lassoregcv = LassoCV(n_alphas=100, normalize=True, random_state=1)\n",
    "lassoregcv.fit(X_entrenamiento, y_entrenamiento)\n",
    "lassoregcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.         -0.28113506  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.         -0.         -0.         -0.15481092  0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.          0.         -0.          0.          0.06451487  0.\n",
      "  0.         -0.          0.         -0.         -0.01920421  0.\n",
      "  0.         -0.          0.03386202  0.          0.          0.08901243\n",
      "  0.         -0.08759757 -0.36986917 -0.         -0.         -0.\n",
      " -0.          0.          0.          0.          0.         -0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.          0.          0.          0.         -0.          0.\n",
      "  0.         -0.          0.          0.01740599 -0.          0.\n",
      " -0.         -0.          0.          0.          0.          0.\n",
      "  0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.          0.         -0.         -0.          0.\n",
      "  0.13471036  0.         -0.         -0.          0.         -0.\n",
      "  0.          0.         -0.          0.          0.          0.\n",
      "  0.0054122   0.         -0.         -0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.          0.\n",
      " -0.          0.          0.02738796  0.         -0.          0.\n",
      " -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Examinamos los coeficientes\n",
    "print (lassoregcv.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16020955801385134\n"
     ]
    }
   ],
   "source": [
    "# El método de predict utiliza el mejor valor alpha\n",
    "y_pred = lassoregcv.predict(X_prueba)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_prueba, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificacion regularizada en scikit-learn\n",
    "\n",
    "Ejemplo: Predecimos el origen del vino utilizando análisis químicos, utilizando el conjunto de datos [wine.data](http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos los datos\n",
    "wine_datos = pd.read_csv('wine.data', header=None)\n",
    "wine_datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    71\n",
       "1    59\n",
       "3    48\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examinamos la variable respuesta\n",
    "wine_datos[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos X e y\n",
    "X = wine_datos.drop(0, axis=1)\n",
    "y = wine_datos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos en conjuntos de entrenamiento y pruebas\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construimos un modelo de regresión logistica no regularizada\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_entrenamiento, y_entrenamiento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.35379897e+00  4.94194975e+00  1.55950755e+01 -2.89427735e+00\n",
      "   1.17114336e-01 -5.47917164e+00  1.06853125e+01  3.07613430e+00\n",
      "  -1.17645188e+01 -2.47195005e+00 -1.79676921e+00  7.33195616e+00\n",
      "   7.12114250e-02]\n",
      " [ 4.89341197e+00 -5.01604946e+00 -1.35320878e+01  1.42295423e+00\n",
      "  -2.16021851e-02  2.88374858e+00  3.83156074e+00  2.01034299e+00\n",
      "   5.06799881e+00 -6.74534304e+00  3.13553248e+00 -6.57041146e+00\n",
      "  -4.38874862e-02]\n",
      " [-9.70207520e-01  2.08269662e+00  9.41526274e-01  2.38024236e-01\n",
      "  -2.49596512e-03 -9.80981464e-01 -6.54889431e+00 -4.83302708e-01\n",
      "  -2.65888441e+00  2.57458634e+00 -1.30417047e+00 -2.34300199e+00\n",
      "   9.48521538e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Examinamos los coeficientes\n",
    "print (logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.50971841e-10 9.87015798e-09 9.99999990e-01]\n",
      " [1.63143584e-17 1.00000000e+00 4.32261192e-10]\n",
      " [9.99999906e-01 4.15338921e-10 9.37653501e-08]\n",
      " [5.36715916e-09 9.99999871e-01 1.24116464e-07]\n",
      " [9.99730855e-01 1.09837026e-18 2.69144834e-04]\n",
      " [3.99183542e-15 7.16896209e-06 9.99992831e-01]\n",
      " [9.99771966e-01 7.18657100e-05 1.56168423e-04]\n",
      " [9.99999720e-01 5.72680165e-21 2.80076725e-07]\n",
      " [4.31923122e-18 2.41595574e-11 1.00000000e+00]\n",
      " [7.51185987e-17 9.99997008e-01 2.99222559e-06]\n",
      " [9.99967664e-01 7.05228153e-11 3.23359700e-05]\n",
      " [9.57542568e-01 4.24572033e-02 2.28811210e-07]\n",
      " [1.02930696e-20 1.00000000e+00 1.12562132e-10]\n",
      " [9.99999998e-01 7.12181208e-12 1.72599671e-09]\n",
      " [2.70137864e-09 9.99999997e-01 3.47692139e-10]\n",
      " [1.74324923e-15 9.99999959e-01 4.13233441e-08]\n",
      " [6.63458456e-30 4.94384777e-01 5.05615223e-01]\n",
      " [9.99842229e-01 1.57744353e-04 2.64241826e-08]\n",
      " [1.28310266e-14 9.99999930e-01 7.00569025e-08]\n",
      " [9.99778349e-01 9.10028885e-19 2.21650519e-04]\n",
      " [9.99999731e-01 2.09710723e-13 2.69236713e-07]\n",
      " [2.23616571e-16 1.00000000e+00 2.86952924e-11]\n",
      " [1.30999415e-09 9.69450541e-01 3.05494573e-02]\n",
      " [6.01971604e-05 9.99939799e-01 3.47406587e-09]\n",
      " [9.99851487e-01 1.48511738e-04 1.09328351e-09]\n",
      " [2.18257529e-13 3.19146724e-07 9.99999681e-01]\n",
      " [9.99998749e-01 1.24837106e-06 3.11780382e-09]\n",
      " [9.99999197e-01 6.55787559e-07 1.47460292e-07]\n",
      " [9.99986850e-01 2.35810311e-12 1.31496994e-05]\n",
      " [3.17041718e-12 1.14073661e-16 1.00000000e+00]\n",
      " [1.40873784e-11 1.00000000e+00 4.08121910e-11]\n",
      " [7.75908522e-32 6.73567584e-16 1.00000000e+00]\n",
      " [3.87974791e-23 2.14286842e-16 1.00000000e+00]\n",
      " [9.99995753e-01 5.86296681e-18 4.24745302e-06]\n",
      " [2.76446118e-01 7.23553881e-01 1.16844824e-09]\n",
      " [2.54434755e-11 9.99999996e-01 3.89756685e-09]\n",
      " [9.91552890e-18 9.99932283e-01 6.77172949e-05]\n",
      " [1.78535234e-03 9.98214633e-01 1.49193078e-08]\n",
      " [1.14640411e-12 9.99999997e-01 3.22321584e-09]\n",
      " [9.99998042e-01 2.55503858e-07 1.70283704e-06]\n",
      " [9.99999019e-01 1.59639405e-10 9.80503854e-07]\n",
      " [1.23064346e-09 9.99903438e-01 9.65610846e-05]\n",
      " [5.79836168e-21 6.87651797e-07 9.99999312e-01]\n",
      " [9.99999984e-01 1.73844906e-09 1.45025309e-08]\n",
      " [9.99999886e-01 3.48591342e-08 7.96364296e-08]]\n"
     ]
    }
   ],
   "source": [
    "# Generamos probabilidades predichas\n",
    "y_pred_prob = logreg.predict_proba(X_prueba)\n",
    "print (y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3129097759404873\n"
     ]
    }
   ],
   "source": [
    "# Calculamos log loss\n",
    "print (metrics.log_loss(y_prueba, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística (regularizada)\n",
    "\n",
    "* Documentación:[sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "* Paramétros:\n",
    "    * C: debe ser positivo, disminuir para obtener más regularización.\n",
    "    * penalty: l1 (lasso) o l2 (ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizamos X_entrenamiento y X_prueba\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "escalador = StandardScaler()\n",
    "escalador.fit(X_entrenamiento)\n",
    "X_entrenamiento_escalado = escalador.transform(X_entrenamiento)\n",
    "X_prueba_escalado = escalador.transform(X_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21048422  0.          0.          0.          0.          0.\n",
      "   0.4871221   0.          0.          0.          0.          0.15337983\n",
      "   1.47738043]\n",
      " [-0.65710356 -0.05626142 -0.11392987  0.          0.          0.\n",
      "   0.          0.          0.         -0.73831077  0.24391052  0.\n",
      "  -0.63405367]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "  -0.84166379  0.          0.          0.61528471 -0.49029539 -0.30483384\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Probamos C=0.1 con penalidad L1 \n",
    "logreg = LogisticRegression(C=0.1, penalty='l1')\n",
    "logreg.fit(X_entrenamiento_escalado, y_entrenamiento)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3622568220367314\n"
     ]
    }
   ],
   "source": [
    "# Generamos probabilidades predichas y calculamos el log loss\n",
    "y_pred_prob = logreg.predict_proba(X_prueba_escalado)\n",
    "print(metrics.log_loss(y_prueba, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59163934  0.06886667  0.33592964 -0.49616684  0.111539    0.21570086\n",
      "   0.40524509 -0.15526139 -0.02534651  0.05399014  0.14877346  0.42327938\n",
      "   0.89815007]\n",
      " [-0.73545676 -0.32942948 -0.47995296  0.294866   -0.1500246   0.04264373\n",
      "   0.14500586  0.07250763  0.17409795 -0.70726652  0.4128986   0.09997212\n",
      "  -0.81284365]\n",
      " [ 0.20136567  0.30989025  0.15977925  0.18867218  0.04204443 -0.27108109\n",
      "  -0.55886639  0.07486943 -0.17471153  0.68266464 -0.52385748 -0.49566967\n",
      "  -0.02565631]]\n"
     ]
    }
   ],
   "source": [
    "# Probamos C=0.1 con penalidad L2\n",
    "logreg = LogisticRegression(C=0.1, penalty='l2')\n",
    "logreg.fit(X_entrenamiento_escalado, y_entrenamiento)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2445883245394477\n"
     ]
    }
   ],
   "source": [
    "# Generamos probabilidades predichas y calculamos el log loss\n",
    "y_pred_prob = logreg.predict_proba(X_prueba_escalado)\n",
    "print (metrics.log_loss(y_prueba, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline y GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline de StandardScaler ay LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'logisticregression__C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]), 'logisticregression__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search para mejores combinaciones C y penalty\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "C_range = 10.**np.arange(-2, 3)\n",
    "penalty_options = ['l1', 'l2']\n",
    "param_grid = dict(logisticregression__C=C_range, logisticregression__penalty=penalty_options)\n",
    "grid = GridSearchCV(pipe, param_grid, cv=10, scoring='neg_log_loss')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00253839, 0.00225835, 0.0022366 , 0.00227954, 0.00247707,\n",
       "        0.00239084, 0.00290232, 0.0024874 , 0.00283737, 0.00263629]),\n",
       " 'std_fit_time': array([0.00143669, 0.00030521, 0.00025845, 0.00027694, 0.00025478,\n",
       "        0.00027788, 0.00030715, 0.00024366, 0.00032974, 0.00036669]),\n",
       " 'mean_score_time': array([0.00096025, 0.00082953, 0.00075326, 0.00073869, 0.00079234,\n",
       "        0.00074708, 0.00078332, 0.00070109, 0.00067472, 0.00065835]),\n",
       " 'std_score_time': array([2.97227755e-04, 1.05725027e-04, 8.65992075e-05, 5.60759332e-05,\n",
       "        1.54123442e-04, 6.63177681e-05, 9.81477092e-05, 1.06496175e-04,\n",
       "        1.27633640e-04, 7.24167970e-05]),\n",
       " 'param_logisticregression__C': masked_array(data=[0.01, 0.01, 0.1, 0.1, 1.0, 1.0, 10.0, 10.0, 100.0,\n",
       "                    100.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_logisticregression__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'logisticregression__C': 0.01,\n",
       "   'logisticregression__penalty': 'l1'},\n",
       "  {'logisticregression__C': 0.01, 'logisticregression__penalty': 'l2'},\n",
       "  {'logisticregression__C': 0.1, 'logisticregression__penalty': 'l1'},\n",
       "  {'logisticregression__C': 0.1, 'logisticregression__penalty': 'l2'},\n",
       "  {'logisticregression__C': 1.0, 'logisticregression__penalty': 'l1'},\n",
       "  {'logisticregression__C': 1.0, 'logisticregression__penalty': 'l2'},\n",
       "  {'logisticregression__C': 10.0, 'logisticregression__penalty': 'l1'},\n",
       "  {'logisticregression__C': 10.0, 'logisticregression__penalty': 'l2'},\n",
       "  {'logisticregression__C': 100.0, 'logisticregression__penalty': 'l1'},\n",
       "  {'logisticregression__C': 100.0, 'logisticregression__penalty': 'l2'}],\n",
       " 'split0_test_score': array([-1.09861229, -0.67839493, -0.49186141, -0.34857883, -0.20405022,\n",
       "        -0.15841465, -0.11922204, -0.09416148, -0.14621738, -0.09099144]),\n",
       " 'split1_test_score': array([-1.09861229, -0.6687004 , -0.44841237, -0.36378914, -0.21253319,\n",
       "        -0.21650974, -0.21563555, -0.20767549, -0.30484463, -0.29990432]),\n",
       " 'split2_test_score': array([-1.09861229e+00, -6.04195192e-01, -3.48167993e-01, -2.26810127e-01,\n",
       "        -5.15828608e-02, -5.58369772e-02, -4.53001102e-03, -1.02531527e-02,\n",
       "        -2.76920193e-04, -1.32930753e-03]),\n",
       " 'split3_test_score': array([-1.09861229, -0.60599184, -0.36038788, -0.25700859, -0.10735944,\n",
       "        -0.11162612, -0.078983  , -0.07381454, -0.09480907, -0.07003004]),\n",
       " 'split4_test_score': array([-1.09861229, -0.62393392, -0.33148282, -0.24546861, -0.06057773,\n",
       "        -0.07455104, -0.0244467 , -0.03332811, -0.02278365, -0.03677034]),\n",
       " 'split5_test_score': array([-1.09861229, -0.63404651, -0.31689496, -0.28131721, -0.07844535,\n",
       "        -0.12069163, -0.05531179, -0.0793459 , -0.05320553, -0.06503056]),\n",
       " 'split6_test_score': array([-1.09861229, -0.64970397, -0.39033974, -0.26648119, -0.06803558,\n",
       "        -0.08154224, -0.02830021, -0.03725848, -0.0238096 , -0.03144182]),\n",
       " 'split7_test_score': array([-1.09861229e+00, -5.91778048e-01, -2.85799869e-01, -2.22566011e-01,\n",
       "        -5.52059610e-02, -7.29172429e-02, -5.95482805e-03, -2.01515748e-02,\n",
       "        -4.37505226e-04, -3.30460163e-03]),\n",
       " 'split8_test_score': array([-1.09861229, -0.58838221, -0.27855335, -0.23355125, -0.06564747,\n",
       "        -0.08591281, -0.03607494, -0.04681482, -0.03000144, -0.03481938]),\n",
       " 'split9_test_score': array([-1.09861229e+00, -6.01915083e-01, -2.76691386e-01, -2.23134321e-01,\n",
       "        -2.42550054e-02, -4.90131246e-02, -1.04288805e-03, -5.78651928e-03,\n",
       "        -3.70497796e-05, -4.82729664e-04]),\n",
       " 'mean_test_score': array([-1.09861229, -0.62546596, -0.35491336, -0.26800817, -0.09431665,\n",
       "        -0.10371211, -0.05804549, -0.06174379, -0.06905478, -0.06443308]),\n",
       " 'std_test_score': array([9.56064504e-17, 3.05811705e-02, 6.93691164e-02, 4.87569529e-02,\n",
       "        6.15243787e-02, 4.90315697e-02, 6.37318048e-02, 5.65671282e-02,\n",
       "        9.11153683e-02, 8.42901346e-02]),\n",
       " 'rank_test_score': array([10,  9,  8,  7,  5,  6,  1,  2,  4,  3], dtype=int32),\n",
       " 'split0_train_score': array([-1.09861229e+00, -6.07506171e-01, -3.24383388e-01, -2.39505804e-01,\n",
       "        -5.51252048e-02, -7.03159411e-02, -7.36845372e-03, -1.68797617e-02,\n",
       "        -8.23851946e-04, -3.20878675e-03]),\n",
       " 'split1_train_score': array([-1.09861229e+00, -6.07093617e-01, -3.23868477e-01, -2.36388930e-01,\n",
       "        -5.38759934e-02, -6.71928815e-02, -6.94233853e-03, -1.54080280e-02,\n",
       "        -7.66104046e-04, -2.74676401e-03]),\n",
       " 'split2_train_score': array([-1.09861229e+00, -6.16525478e-01, -3.36579166e-01, -2.48647139e-01,\n",
       "        -6.02199152e-02, -7.56886806e-02, -8.36063408e-03, -1.90938163e-02,\n",
       "        -9.21838646e-04, -3.75781992e-03]),\n",
       " 'split3_train_score': array([-1.09861229e+00, -6.16828657e-01, -3.32846841e-01, -2.45476974e-01,\n",
       "        -5.72384287e-02, -7.21528200e-02, -7.74391449e-03, -1.74891011e-02,\n",
       "        -9.13638123e-04, -3.45341479e-03]),\n",
       " 'split4_train_score': array([-1.09861229e+00, -6.16061576e-01, -3.38224342e-01, -2.48253509e-01,\n",
       "        -6.10051888e-02, -7.52206752e-02, -8.25320874e-03, -1.89651767e-02,\n",
       "        -9.19088886e-04, -3.69778542e-03]),\n",
       " 'split5_train_score': array([-1.09861229e+00, -6.11182667e-01, -3.37498222e-01, -2.44309958e-01,\n",
       "        -5.84388268e-02, -7.23505991e-02, -7.95511836e-03, -1.76890788e-02,\n",
       "        -9.34087962e-04, -3.54189080e-03]),\n",
       " 'split6_train_score': array([-1.09861229e+00, -6.17697653e-01, -3.36859748e-01, -2.48390237e-01,\n",
       "        -6.08805688e-02, -7.49835762e-02, -8.22348709e-03, -1.86585551e-02,\n",
       "        -9.83817434e-04, -3.63510929e-03]),\n",
       " 'split7_train_score': array([-1.09861229e+00, -6.17815942e-01, -3.39721633e-01, -2.48129996e-01,\n",
       "        -5.98400977e-02, -7.46083913e-02, -8.31344026e-03, -1.91704743e-02,\n",
       "        -9.58834244e-04, -3.88530295e-03]),\n",
       " 'split8_train_score': array([-1.09861229e+00, -6.13648448e-01, -3.34074446e-01, -2.44907443e-01,\n",
       "        -5.98023901e-02, -7.34233374e-02, -8.11945705e-03, -1.85427290e-02,\n",
       "        -9.46195991e-04, -3.71642289e-03]),\n",
       " 'split9_train_score': array([-1.09861229e+00, -6.16711363e-01, -3.36670164e-01, -2.48189554e-01,\n",
       "        -6.12336377e-02, -7.56347339e-02, -8.34700128e-03, -1.94596177e-02,\n",
       "        -9.18684203e-04, -3.89238626e-03]),\n",
       " 'mean_train_score': array([-1.09861229e+00, -6.14107157e-01, -3.34072643e-01, -2.45219955e-01,\n",
       "        -5.87660252e-02, -7.31571636e-02, -7.96270536e-03, -1.81356339e-02,\n",
       "        -9.08614148e-04, -3.55356831e-03]),\n",
       " 'std_train_score': array([0.00000000e+00, 3.89909012e-03, 5.30272827e-03, 3.99894907e-03,\n",
       "        2.43976092e-03, 2.60042621e-03, 4.52573363e-04, 1.20431395e-03,\n",
       "        6.17607611e-05, 3.31739039e-04])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimimos las puntuaciones log loss\n",
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.058045485199439396\n",
      "{'logisticregression__C': 10.0, 'logisticregression__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "# Examinamos el mejor modelo\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de modelos lineales regularizados con modelos lineales no regularizados\n",
    "\n",
    "* Ventajas de los modelos lineales regularizados:\n",
    "\n",
    "    - Mejor desempeño.\n",
    "    - La regularización L1 realiza la selección automática de características.\n",
    "    - Útil para problemas de gran dimensión ($p> n$).\n",
    "\n",
    "* Desventajas de los modelos lineales regularizados:\n",
    "\n",
    "    - El ajuste es obligatorio\n",
    "    - Se recomienda escalar características\n",
    "    - Menos interpretable (debido al escalado de características)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
