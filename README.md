# Curso CM-072

Curso elemental de python y las librerías scikit learn y keras aplicados al machine learning.

## Introducción 

* Cálculo
  - Apendice D del libro de Chris Bishop.
  - [Notas](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/readings/lagrange.pdf) del MIT para multiplicadores de Lagrange.
  - [Lagrange Multipliers without Permanent Scarring](https://people.eecs.berkeley.edu/~klein/papers/lagrange-multipliers.pdf) de  Dan Klein.
  
* Probabilidad
  
  - [Notas ](http://www.statslab.cam.ac.uk/~rrw1/prob/prob-weber.pdf) de  Richard Weber.
  - Capitulo 2 del libro de Kevin P. Murphy o Chris Bishop.
  - [Notas](http://cs229.stanford.edu/section/cs229-prob.pdf) de probabilidades de las clases de Machine Learning de Stanford.
 
* Álgebra Lineal
  - [Coding The Matrix: Linear Algebra Through Computer Science Applications](http://codingthematrix.com/), fantástico libro de Philip Klein (Revisar los diapositivas que acompañan al libro).
  - [Notas](http://cs229.stanford.edu/section/cs229-linalg.pdf) de álgebra lineal de las clases de Machine Learning de Stanford.
  - Apendice C del libro de Chris Bishop.
  - [Notas ](http://cs.nyu.edu/%7Edsontag/courses/ml12/notes/linear_algebra.pdf) de Sam Roweis.
  
* Optimización
  - [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) de Stephen Boyd y  Lieven Vandenberghe.
  - Notas de Optimización de las clases de Machine Learning de Stanford:
    - [Convex Optimization Overview 1](http://cs229.stanford.edu/section/cs229-cvxopt.pdf).
    - [Convex Optimization Overview 2](http://cs229.stanford.edu/section/cs229-cvxopt2.pdf).
 

### El mundo del machine learning y la IA

* Ejemplo 1: [Introduction](https://www.youtube.com/watch?v=i8D90DkCLhI).
* Ejemplo 2: [Rules on Rules on Rules](https://www.youtube.com/watch?v=2ZhQkD1QKFw).
* Ejemplo 3: [Now I R1](https://www.youtube.com/watch?v=0cRXaORbIFA).
* Ejemplo 4: [Machine Learning](https://www.youtube.com/watch?v=sarVw-iVWgc).
* Ejemplo 5: [To Learn is to Generalize](https://www.youtube.com/watch?v=efR8ybG7Ihs).
* Ejemplo 6: [It's Definitely Time to Play with Legos](https://www.youtube.com/watch?v=GufQYkMkdpw).
* Ejemplo 7: [There is no f](https://www.youtube.com/watch?v=klWUOO4sHaA).
* Ejemplo 8: [More Assumptions...Fewer Problems?](https://www.youtube.com/watch?v=UVwwYZMFocg).
* Ejemplo 9: [Bias Variance Throwdown](https://www.youtube.com/watch?v=ZYjCIazhKbk).
* Ejemplo 10: [World Domination](https://www.youtube.com/watch?v=6cvPj9dmYTo).
* Ejemplo 11: [Haystacks on Haystacks](https://www.youtube.com/watch?v=biy2yU3Auc4).
* Ejemplo 12: [Let's Get Greedy](https://www.youtube.com/watch?v=Kg8W_q8pHik).
* Ejemplo 13: [Heuristics](https://www.youtube.com/watch?v=g_sA8hYU3b8).
* Ejemplo 14: [Mejorando las Heurísticas](https://www.youtube.com/watch?v=tPHImr2sFBM).
* Ejemplo 15: [Information](https://www.youtube.com/watch?v=FMCY3SXTELE).

###  Material de referencia

* Libros de Machine Learning
  - Data Science From Scratch: First Principles with Python de Joel Grus 2015.
  - Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, Aurélien Géron  O'Reilly Media; 1 edition 2017.
  - [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) de Chris Bishop  (2006). 
 
## Horario de clases 

* Miércoles 3- 6  J3162B.
* Sábados  2- 5 Sala 1.

# Evaluación 

La nota final se obtiene de la siguiente manera:

Nota Final = (0.7* Examen Parcial + 0.3 * Trabajo1  + 0.5 * Examen Final + 0.5 * Trabajo2 +  Tareas)/3

- Sólo se tomarán las mejores 6 tareas como notas de prácticas calificadas.

## Esquema de trabajo para el curso CM-072

- Trabajos individuales.
- Entregable:
  - Archivo PDF (Informe).
  - Cuaderno de Jupyter con el código y un resumen del proyecto.
  - Presentación en Beamer  para su exposición.
  
El trabajo del curso será la aplicación de modelos de Machine Learning en una tarea de clasificación y en el posible de alguna técnica de regresión o alguna otra relacionada al aprendizaje no supervisado. Se requerirá el uso de al menos 4 métodos de clasificación de su elección y la evaluación del rendimiento en una tarea común, así como establecer una comparación con una línea base reportada previamente en otros artículos.

### Conjunto de datos:

- [Netflix Prize data](https://www.kaggle.com/netflix-inc/netflix-prize-data). Dataset from Netflix's competition to improve their reccommendation algorithm.
- [Cars Overhead With Context (COWC)](https://gdo152.llnl.gov/cowc/). Data set of a large set of annotated cars from overhead.
    * [Car Localization and Counting with Overhead Imagery, an Interactive](https://medium.com/the-downlinq/car-localization-and-counting-with-overhead-imagery-an-interactive-exploration-9d5a029a596b). 
    * [Vehicle Detection in Aerial Images](https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-5431/9230_read-42467/).
    * [Training a deep-learning classifier for aerial top view detection of vehicles](https://medium.com/@ckyrkou/training-a-deep-learning-classifier-for-aerial-top-view-detection-of-vehicles-874f88d81c4).
    * [Monza: Image Classification of Vehicle Make and Model Using Convolutional  Neural Networks and Transfer Learning](http://cs231n.stanford.edu/reports/2015/pdfs/lediurfinal.pdf).
- [Lending Club Loan data](https://www.kaggle.com/wendykan/lending-club-loan-data). Analyze Lending Club's issued loans.
- [The Enron Email Dataset](https://www.kaggle.com/wcukierski/enron-email-dataset). 500,000+ emails from 150 employees of the Enron Corporation.
- [Dota 2 Matches](https://www.kaggle.com/devinanzelmo/dota-2-matches). Explore player behavior and predict match outcomes.
- [UK Road Safety: Traffic Accidents and Vehicles](https://www.kaggle.com/tsiaras/uk-road-safety-accidents-and-vehicles).Detailed dataset of road accidents and involved vehicles in the UK (2005-2016).
- [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/). Database of face photographs designed for studying the problem of unconstrained face recognition.
    * [Face Recognition using Tensorflow](https://github.com/davidsandberg/facenet).
    * [CNNs for Face Detection and Recognition](http://cs231n.stanford.edu/reports/2017/pdfs/222.pdf).
    * [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf).
    * [Building a real time Face Recognition system using pre-trained FaceNet model](https://medium.com/@vinayakvarrier/building-a-real-time-face-recognition-system-using-pre-trained-facenet-model-f1a277a06947).
- [Mobile phone activity in a city](https://www.kaggle.com/marcodena/mobile-phone-activity). Hourly phone calls, SMS and Internet communication of an entire city.
- [New York Times Comments](https://www.kaggle.com/aashita/nyt-comments).Comments on articles published in the New York Times.
- [GTZAN music/speech collection](https://www.kaggle.com/lnicalo/gtzan-musicspeech-collection).Dataset collected for the purposes of music/speech discrimination.
    * [Deep Music Genre](http://cs231n.stanford.edu/reports/2017/pdfs/22.pdf).
    * [Convolutional Recurrent Neural network for music classification](https://arxiv.org/pdf/1609.04243.pdf).
    * [Music Genre Classification using Machine Learning Techniques](https://arxiv.org/pdf/1804.01149.pdf).

###  **Evaluación** (Adaptado del trabajo del Dr. Ceśar Beltrán Castañón y  Mg. César Olivares Poggi)

La evaluación serán 5 entregables, lo cuales se detallan a continuación, que serán parte de las calificaciones de laboratorios y y exámenes y no son anulables.

| **Entregable** | **Fecha límite de entrega** |
| --- | --- |
| 1.  Propuesta de proyecto |5 de setiembre |
| 2. (Introducción, estado del arte y diseño del experimento), en formato IEEE, tamaño A4, máximo 3 páginas de extensión. | 15 de setiembre |
| 3. Código y/o scripts (jupyter notebooks) con la experimentación realizada | 20 de octubre |
| 4. Informe final, en formato IEEE, tamaño A4, máximo 10 páginas de extensión |10 de noviembre|
| 5. Exposición de presentación de trabajo | 14 de diciembre|

Se tomará más énfasis en  lo siguiente  para las evaluaciones:

- Contenido del Informe.
- La solución al problema a realizar y la mención de las variantes.
- Exposición y presentación de posters.

### **Propuesta de proyecto**

La propuesta de proyecto deberá incluir lo siguiente (máximo una hoja A4):

- Nombre del proyecto
- Proyecto a utilizar (dependiendo del orden de trabajos se hará las asignaciones repetidas)
- Objetivo del proyecto
- Artículos científicos relevantes
-  Propuesta tentativa de modelos a utilizar.

### **Informe del trabajo de curso:**

El informe del proyecto deberá explicar claramente el objetivo del estudio, trabajos previos sobre el problema, código de solución (varias opciones) del problema en cuestión, pruebas, conclusiones, etc.  Para la mayoría de los problemas los paquetes de software de  Python están disponibles como dominio público. No hay restricción para usarlos.

El informe debe incluir la siguiente información:

- Introducción
  - Presentación del problema general sobre el que versará el trabajo y cómo se integra dentro del uso del lenguaje python y del curso.
  - Objetivo del estudio
  - Organización del informe (secciones).
-  Estado del arte
  - Breve mención del aporte que otros artículos científicos han realizado para este problema
  -  Mención de al menos 3 artículos científicos que mencionen el problema y las variantes realizadas

- Diseño del experimento
  - Descripción de los objetos, funciones y técnicas a utilizar.

- Experimentos y resultados
  - Línea base: Reproducción de resultados reportados en un artículo científico anterior
  - Evaluación del rendimiento de los modelos ensayados
  - Comparación de línea base y resultados propios

- Discusión
  - Interpretación de los resultados obtenidos
  -  ¿Cómo podría ser mejorado sus resultados?

- Conclusiones y trabajos futuros.

- Bibliografía o Referencias.


**Código y/o scripts (Jupyter Notebooks )**

- El código será trabajado de forma individual  utilizando  GitHub, de manera que se pueda verificar los aportes hechos por cada uno de los integrantes del curso.
- Se recomienda trabajar con jupyter notebooks o Jupyter lab en Python usando las librerías del curso. 
- Se deberá asignar nombres representativos a los archivos, de manera que se pueda identificar su orden relativo y el propósito de cada uno. No hay restricciones para tomar como base código tomado de otras fuentes, siempre y cuando se cite debidamente la fuente y se realice las adaptaciones que requiera el propio trabajo.
- El código deberá estar mínimamente comentado, siempre en español. Se ignorará cualquier comentario en otro idioma.
- Asimismo, se ignorará cualquier código simplemente copiado cuya fuente no haya sido citada, y se asignará el puntaje mínimo al estudiante.

### **Exposición**

El trabajo final, tendrá exposiciones. Se recomienda seguir las guías indicadas abajo:

### **Recursos**

- Git y Github | Curso Práctico de Git y Github Desde Cero--> [https://www.youtube.com/watch](https://www.youtube.com/watch)[v=HiXLkL42tMU](https://www.youtube.com/watch?v=HiXLkL42tMU).
- Editor colaborativo LaTeX en línea-->[https://www.overleaf.com/](https://www.overleaf.com/).

- **Formato IEEE (MS Word y LaTeX)** -->[**https://www.ieee.org/conferences/publishing/templates.html**](https://www.ieee.org/conferences/publishing/templates.html).

- Buscador de literatura académica-->[**https://scholar.google.com.pe/**](https://scholar.google.com.pe/).

- Jupyter Notebook Tutorial: The Definitive Guide-->[**https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook**](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook).

- Guías para elaborar un poster:

  - [https://guides.nyu.edu/posters](https://guides.nyu.edu/posters)
  - [https://nau.edu/undergraduate-research/poster-presentation-tips/](https://nau.edu/undergraduate-research/poster-presentation-tips/).
  - [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1876493/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1876493/).


### Clase 1: 8 de setiembre
- Andrew ng y sus consejos en ML en el artículo : [ML-advice](http://cs229.stanford.edu/materials/ML-advice.pdf).
- Cuaderno-->[Introducción al machine learning](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/01_IntroduccionML.ipynb).
- Datos-->[Iris Data Set](http://archive.ics.uci.edu/ml/datasets/Iris).
- Lectura-->[API design for machine learning software:experiences from the scikit-learn project](https://arxiv.org/pdf/1309.0238.pdf).
- Cuaderno-->[Introducción al aprendizaje supervisado](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/Aprendizaje_supervisado/02_AprendizajeSupervisado.ipynb).
- Lectura-->[Overfitting vs. Underfitting: A Conceptual Explanation](https://towardsdatascience.com/overfitting-vs-underfitting-a-conceptual-explanation-d94ee20ca7f9).
- Lectura -->[Clever Methods of Overfitting](http://hunch.net/?p=22).
- Lectura-->[A Detailed Introduction to K-Nearest Neighbor (KNN) Algorithm](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/).
- Lectura opcional-->[What is data science? The future belongs to the companies and people that turn data into products.](https://www.oreilly.com/ideas/what-is-data-science).
- Lectura opcional-->[KDD, SEMMA AND CRISP-DM: A parallel overview](http://recipp.ipp.pt/bitstream/10400.22/136/3/KDD-CRISP-SEMMA.pdf).
- Diapositiva-->[Presentación 1](https://github.com/C-Lara/Curso-CM072/blob/master/Presentaciones/PresentacionML1.pdf).
- [Tarea 1](https://github.com/C-Lara/Curso-CM072/blob/master/Tareas/Tarea1.ipynb)-->Presentación 12 de setiembre.

### Clase 2 : 15 de setiembre 

 - Cuaderno-->[Introducción a lo modelos lineales](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/03_ModelosLineales.ipynb).
 - Lectura-->[Four Assumptions Of Multiple Regression That Researchers Should Always
Test](https://pareonline.net/getvn.asp?v=8&n=2).
- Lectura-->[Generalized Linear Models](http://scikit-learn.org/stable/modules/linear_model.html).
 - Cuaderno-->[Modelos lineales para clasificación](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/Modelos_lineales_clasificacion/04_ModelosLineales_Clasificacion.ipynb).
 - Lectura-->[The Naive Bayes Classifier](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c).
 - Lectura-->[The Logistic Regression Algorithm](https://towardsdatascience.com/the-logistic-regression-algorithm-75fe48e21cfa).
  - Lectura opcional-->[The End of Theory: The Data Deluge Makes the Scientific Method Obsolete](https://www.wired.com/2008/06/pb-theory/).
 - Lectura opcional-->[How to write a production-level code in Data Science?](https://towardsdatascience.com/how-to-write-a-production-level-code-in-data-science-5d87bd75ced).
 - [Tarea 2](https://github.com/C-Lara/Curso-CM072/blob/master/Tareas/Tarea2/Tarea2.ipynb)-->Presentación 22 de setiembre..
 
 ### Clase 3 : 22 de setiembre
 -  Lectura-->[Random Test/Train Split is not Always Enough](http://www.win-vector.com/blog/2015/01/random-testtrain-split-is-not-always-enough/).
  - Cuaderno-->[Métricas de evaluación](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/Metricas/20_Metricas.ipynb).
 - Cuaderno-->[Curvas de exhaustividad- precisión, curvas ROC, AUC](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/21_Curvas_precision_ROC.ipynb).
 - Lectura -->[An introduction to ROC analysis](http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf).
 - Cuaderno-->[Estimación de clasificadores](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/Estimacion_incertidumbre_clasificadores/09_Estimacion_Incertidumbre_clasificadores.ipynb).
 - Lectura-->[Comparing supervised learning algorithms](http://www.dataschool.io/comparing-supervised-learning-algorithms/) 
 - [Tarea 3](https://github.com/C-Lara/Curso-CM072/blob/master/Tareas/Tarea3/Tarea3.ipynb)-->Presentación 29 de setiembre.
 
 ### Clase 4 : 29 de setiembre
  - Lectura inicial-->[Feature Engineering: The key to predictive modeling](https://medium.com/@karanrajwanshi/feature-engineering-the-key-to-predictive-modeling-8f1935b3db4f).
 - Cuaderno-->[Ingeniería de características](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/Ingenieria_caracteristicas/16_Ingenieria_caracteristicas.ipynb).
 - Lecturas:
    * [Understanding Feature Engineering-Continuous Numeric](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b).
    * [Understanding Feature Engineering-Categorical Data](https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63).
 - [Tarea 4](https://github.com/C-Lara/Curso-CM072/blob/master/Tareas/Tarea4/Tarea4.ipynb)-->Presentación 3 de octubre.
  

 ### Clase 5 : 3 de octubre
  - Lectura -->[A Brief History of Classification and Regression Trees](https://drive.google.com/file/d/0B-BKohKl-jUYQ3RpMEF0OGRUU3RHVGpHY203NFd3Z19Nc1ZF/view).
  - Cuaderno-->[Árboles de decisión](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/Arbol_decision/05_ArbolesDecision.ipynb).
 - Lectura -->[Decision  tree learning](http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf).
 - [Tarea]().

### Clase 6 : 6 de octubre
- Repaso-->[Gradient, Subgradient and how they may affect your grade](http://cs.nyu.edu/~dsontag/courses/ml16/slides/notes_convexity16.pdf).
 - Lectura-->[Support Vector Machines](http://cs229.stanford.edu/notes/cs229-notes3.pdf).
 - Cuaderno-->[SVM](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/SVM/07_SVM.ipynb).
 - Lectura -->[The Kernel Trick](https://dscm.quora.com/The-Kernel-Trick).
 - Resumen-->[SVM Revisit](http://www.fuzihao.org/blog/2017/11/18/SVM-Revisit/).
 - [Tarea]().


 ### Clase 7 : 13 de octubre
  - Cuaderno -->[Selección automática de características](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/17_Seleccion_caracteristicas.ipynb).
 - Lectura-->[Model evaluation: quantifying the quality of predictions](http://scikit-learn.org/stable/modules/model_evaluation.html).
 - Cuaderno -->[Evaluación de modelos](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/Evaluacion_modelos/18_Evaluacion_modelos.ipynb).
  - Cuaderno-->[Validación cruzada](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/19_Validacion_cruzada.ipynb).
 - Lectura-->[Cross-validation pitfalls when selecting and assessing regression and classification models](https://jcheminf.biomedcentral.com/track/pdf/10.1186/1758-2946-6-10).
 - [Tarea]().
 
 ### Clase 8 : 20 de octubre 
  - Cuaderno-->[Ensamblado de árboles de decisión](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/06_Ensamble_ArbolesDecision.ipynb).
 - Lectura-->[Gradient Boosting Explained](https://gormanalysis.com/gradient-boosting-explained/).
 - Lectura opcional-->[Understanding Random Forest from theory to practice](https://arxiv.org/pdf/1407.7502v3.pdf).
 - Lectura -->[XGBoost: A Scalable Tree Boosting System](http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)

  
 ### Clase 9 : 3 de noviembre
 - Cuaderno-->[Aprendizaje no supervisado](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/10_Aprendizaje_noSupervisado.ipynb).
 - Cuaderno-->[PCA](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/11_PCA.ipynb).
 - Video-->[Acerca de PCA](https://www.youtube.com/watch?v=kw9R0nD69OU).
 - Cuaderno opcional-->- Cuaderno-->[NFE y Manifold learning](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/12_NMF.ipynb).
 - Lectura-->[A Tutorial on Principal Component Analysis](https://arxiv.org/pdf/1404.1100.pdf).
 - [Tarea]().
 
 ### Clase 11 : 10 de noviembre
  - Cuaderno-->[Clustering](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/13_Clustering.ipynb).
  - Lectura opcional-->[Data Clustering: A Review](http://eprints.library.iisc.ernet.in/273/1/p264-jain.pdf).
  - Cuaderno-->[Clustering aglomerativo](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/14_Clustering_aglomerativo.ipynb).
  - Interacción-->[Hierarchical Clustering in Action](https://joyofdata.shinyapps.io/hclust-shiny/).
  - Lectura-->[Tipos de clustering en scikit-learn](http://scikit-learn.org/stable/modules/clustering.html).
  - Cuaderno-->[Comparación y evaluación de clustering](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/15_Comparacion_evaluacion_clustering.ipynb).
  - [Tarea]().
 
 ### Clase 12 : 17 de noviembre 
 
 - Cuaderno-->[Pipeline](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/22_Pipeline.ipynb).
 - Cuaderno-->[Interfaz general de los pipelines](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/Interfaz_pipeline/23_Interfaz_pipeline.ipynb).
 - [Tarea]()
 

 ### Clase 13: 24 de noviembre
 - Cuaderno-->[Redes neuronales](https://nbviewer.jupyter.org/github/C-Lara/Curso-CM072/blob/master/Cuadernos-CM072/08_RedesNeuronales.ipynb).
  - Lectura-->[Deep learning](http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf).
  - Lectura opcional-->[An overview of gradient descent optimizationalgorithms](https://arxiv.org/pdf/1609.04747.pdf).
  - [Tarea]().
  
  ### Clase 15: 1 de diciembre
  - Introducción a Keras.
  - [Tarea]().
 

## Herramientas a  usar 

### Anaconda 

[Anaconda](https://www.continuum.io/downloads) es una distribución completa  libre de [Python](https://www.python.org/) incluye [paquetes de Python ](http://docs.continuum.io/anaconda/pkg-docs).

Anaconda incluye los instaladores de Python 2.7 y 3.5.  La instalación en **Linux**, se encuentra en la página de Anaconda y es 
más o menos así

1 . Descargar el instalador de Anaconda para Linux.

2 . Después de descargar el instalar, en el terminal, ejecuta para 3.5

```bash
c-lara@Lara:~$ bash Anaconda3-2.4.1-Linux-x86_64.sh

```

Es recomendable leer, alguna de las característica de Anaconda en el siguiente material [conda 30-minutes test drive](http://conda.pydata.org/docs/test-drive.html).

3 . La instalación de paquetes como [seaborn](http://stanford.edu/~mwaskom/software/seaborn/) o [bokeh](http://bokeh.pydata.org/en/latest/) se pueden realizar a través de Anaconda, de la siguiente manera:



``` bash
c-lara@Lara:~$ conda install bokeh
```
Alternativamente podemos desde PyPI usando **pip**:

```bash
c-lara@Lara:~$ pip install bokeh
``` 

El proyecto [Anaconda](https://www.continuum.io/downloads) ha creado [R Essentials](http://anaconda.org/r/r-essentials), que incluye el IRKernel y alrededor de 80 paquetes para análisis de datos, incluyendo `dplyr`, `shiny`, `ggplot2`,`caret`, etc. Para instalar **R Essentials** en un entorno de trabajo, hacemos

```bash
c-lara@Lara:~$ conda install -c r r-essentials
``` 

### Proyecto Jupyter y el Jupyter Nbviewer

El [Proyecto Jupyter](http://jupyter.org/)  es una aplicación web que te permite crear y compartir documentos que contienen código de diversos lenguajes de programación, ecuaciones,  visualizaciones y texto en diversos formatos. El uso de Jupyter incluye la ciencia de datos, simulación numérica, la modelización en estadística, Machine Learning, etc.


[Jupyter nbviewer](https://nbviewer.jupyter.org/)  es un servicio web gratuito que te permite compartir las versiones de archivos realizados por Jupyter, permitiendo el renderizado de diversos fórmatos incluyendo, código latex.

- [Jupyter Documentation](https://jupyter.readthedocs.io/en/latest/).

[Unofficial Jupyter Notebook Extensions](http://jupyter-contrib-nbextensions.readthedocs.io/en/latest/) contiene una colección de extensiones no oficiales de la comunidad que añaden funcionalidad a Jupyter notebook. Estas extensiones están escritas principalmente en Javascript y se cargarán localmente en su navegador.

```bash
c-lara@Lara:~$ pip install jupyter_contrib_nbextensions
``` 

O utilizando conda

```bash
c-lara@Lara:~$ conda install -c conda-forge jupyter_contrib_nbextensions
``` 

## Software

* [NumPy](http://www.numpy.org/), es la biblioteca natural para  python numérico. La característica más potente de NumPy es la  matriz n-dimensional. Esta biblioteca  contiene funciones básicas de álgebra lineal, transformadas de Fourier, capacidades avanzadas de números aleatorios y herramientas para la integración con otros lenguajes de bajo nivel como Fortran, C y C ++.

* [SciPy](https://www.scipy.org/) es la biblioeteca para python científico. SciPy se basa en NumPy y es una de las bibliotecas más útiles por la variedad de módulos de ciencia y ingeniería de alto nivel con la que cuenta, como la transformada discreta de Fourier,  álgebra lineal, optimización,  matrices dispersas, etc.

* [Matplotlib](http://matplotlib.org/) es una librería de Python  para  crear una gran variedad de gráficos, a partir de histogramas, lineas, etc, usando si es necesario  comandos de látex para agregar matemáticas a los gráficos.

* [Pandas](http://pandas.pydata.org/) es una librería  para operaciones y manipulaciones de datos estructurados. Pandas ha sido añadido  recientemente a Python y han sido fundamental para impulsar el uso de Python en la ciencia de datos.

* [Scikit-learn](http://scikit-learn.org/stable/), es tal vez la mejor biblioteca para Machine Learning, construida sobre NumPy, SciPy y Matplotlib, esta biblioteca contiene una gran cantidad de herramientas eficientes para el Machine Learning y el modelado estadístico incluyendo clasificación, regresión, agrupación y reducción de la dimensionalidad.

* [Seaborn](https://seaborn.pydata.org/) es una libreria para la visualización de datos estadísticos. Seaborn es una biblioteca para hacer atractivos e informativos los gráficos estadísticos en Python. Se basa en matplotlib. Seaborn pretende hacer de la visualización una parte central de la exploración y la comprensión de los datos.

* [keras](https://keras.io/) es una librería de redes neuronales de código abierto escrito en Python.


### Git y Github

[Git](https://git-scm.com/) es un sistema de control de versiones de gran potencia y versatilidad en el manejo de un gran número de archivos de  código fuente a a través del desarrollo no lineal, es decir vía la gestión rápida de ramas y mezclado de Para poder revisar y aprender los comandos necesarios de Git, puedes darle una ojeada al excelente [tutorial de CodeSchool](https://try.github.io/levels/1/challenges/1) o a la [guía](http://rogerdudler.github.io/git-guide/index.es.html) de Roger Dudle para aprender  Git.

[Github](https://github.com/) es una plataforma de desarrollo colaborativo de software utilizado para alojar proyectos (muchos proyectos importantes como paquetes de R, Django, el Kernel de Linux, se encuentran alojados ahí) utilizando Git y el framework Ruby on Rails.

Podemos instalar Git en Ubuntu utilizando el administrador de paquetes `Apt`:

```bash
c-lara@Lara:~$sudo apt-get update
c-lara@Lara:~$sudo apt-get install git
```

Referencias y Lecturas:

- [Usando el GIT](http://www.cs.swarthmore.edu/~newhall/unixhelp/git.php).
- [Practical Git Introduction](http://marc.helbling.fr/2014/09/practical-git-introduction).
- [Visual Git Guide](http://marklodato.github.io/visual-git-guide/index-es.html).
